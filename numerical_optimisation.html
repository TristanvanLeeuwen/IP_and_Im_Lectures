

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>6. Numerical optimisation for inverse problems &#8212; 10 Lectures on Inverse Problems and Imaging</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/mystnb.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Image processing" href="image_processing.html" />
    <link rel="prev" title="5. Variational formulations for inverse problems" href="variational_formulations.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">10 Lectures on Inverse Problems and Imaging</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="what_is.html">
   1. What is an inverse problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_ip_regularization.html">
   2. Discrete Inverse Problems and Regularisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ip_function_spaces.html">
   3. Linear inverse problems in function spaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistical_perspective.html">
   4. A statistical perspective on inverse problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="variational_formulations.html">
   5. Variational formulations for inverse problems
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Numerical optimisation for inverse problems
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="image_processing.html">
   1. Image processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tomography.html">
   2. Computed Tomography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wavefield_imaging.html">
   3. Wavefield Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="magnetic_resonance_imaging.html">
   4. Magnetic Resonance Imaging
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/numerical_optimisation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/TristanvanLeeuwen/IP_and_Im_Lectures/edit/master/numerical_optimisation.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#smooth-optimisation">
   6.1. Smooth optimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     6.1.1. Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linesearch">
     6.1.2. Linesearch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#second-order-methods">
     6.1.3. Second order methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-optimisation">
   6.2. Convex optimisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subgradient-descent">
     6.2.1. Subgradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proximal-gradient-methods">
     6.2.2. Proximal gradient methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#splitting-methods">
     6.2.3. Splitting methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   6.3. References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   6.4. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#steepest-descent-for-strongly-convex-functionals">
     6.4.1. Steepest descent for strongly convex functionals
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#steepest-descent-for-convex-functions">
     6.4.2. Steepest descent for convex functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rosenbrock">
     6.4.3. Rosenbrock
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subdifferentials">
     6.4.4. Subdifferentials
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dual-problems">
     6.4.5. Dual problems
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tv-denoising">
     6.4.6. TV-denoising
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assignments">
   6.5. Assignments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spline-regularisation">
     6.5.1. Spline regularisation
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="numerical-optimisation-for-inverse-problems">
<h1><span class="section-number">6. </span>Numerical optimisation for inverse problems<a class="headerlink" href="#numerical-optimisation-for-inverse-problems" title="Permalink to this headline">¶</a></h1>
<p>In this chapter we treat numerical algorithms for solving optimisation problems over <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Throughout we will assume that the objective <span class="math notranslate nohighlight">\(J(u) = D(u) + R(u)\)</span> satisfies the conditions for a unique minimiser to exist. We distinguish between two important classes of problems; <em>smooth</em> problems and <em>convex</em> problems.</p>
<div class="section" id="smooth-optimisation">
<h2><span class="section-number">6.1. </span>Smooth optimisation<a class="headerlink" href="#smooth-optimisation" title="Permalink to this headline">¶</a></h2>
<p>For smooth problems, we assume to have access to as many derivatives of <span class="math notranslate nohighlight">\(J\)</span> as we need. As before, we denote the first derivative (or gradient) by <span class="math notranslate nohighlight">\(J' : \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span>. We denote the second derivative (or Hessian) by <span class="math notranslate nohighlight">\(J'' : \mathbb{R}^n \rightarrow \mathbb{R}^{n\times n}\)</span>. We will additionally assume that the Hessian is globally bounded, i.e. there exists a constant <span class="math notranslate nohighlight">\(L &lt; \infty\)</span> such that <span class="math notranslate nohighlight">\(J''(u) \preceq L\cdot I\)</span> for all <span class="math notranslate nohighlight">\(u\in\mathbb{R}^n\)</span>. Note that this implies that <span class="math notranslate nohighlight">\(J'\)</span> is Lipschitz continous with constant <span class="math notranslate nohighlight">\(L\)</span>: <span class="math notranslate nohighlight">\(\|J'(u) - J'(v)\|_2 \leq L \|u - v\|_2\)</span>.</p>
<p>For a comprehensive treatment of this topic (and many more), we recommend the seminal book <em>Numerical Optimization</em> by Stephen Wright and Jorge Nocedal <a class="bibtex reference internal" href="#nocedal2006numerical" id="id1">[7]</a>.</p>
<hr class="docutils" />
<p>Before discussing optimisation methods, we first introduce the optimality conditions.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Optimality conditions</em></p>
<p>Given a smooth functional <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\rightarrow \mathbb{R}\)</span>, a point <span class="math notranslate nohighlight">\(u_* \in \mathbb{R}^n\)</span> is local minimiser iff it satisfies the first and second order optimality conditions</p>
<div class="math notranslate nohighlight">
\[J'(u_*) = 0, \quad J''(u_*) \succeq 0.\]</div>
<p>If <span class="math notranslate nohighlight">\(J''(u_*) \succ 0\)</span> we call <span class="math notranslate nohighlight">\(u_*\)</span> a <em>strict</em> local minimiser.</p>
</div>
<div class="section" id="gradient-descent">
<h3><span class="section-number">6.1.1. </span>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>The steepest descent method proceeds to find a minimiser through a fixed-point iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I - \lambda J'\right)(u_k) = u_k - \lambda J'(u_k),\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> is the stepsize. The following theorem states that this iteration will yield a fixed point of <span class="math notranslate nohighlight">\(J\)</span>, regardless of the initial iterate, provided that we pick <span class="math notranslate nohighlight">\(\lambda\)</span> small enough.</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Global convergence of steepest descent</em></p>
<p>Let <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\rightarrow \mathbb{R}\)</span> be a smooth, Lipschitz-continuos functional. The fixed point iteration</p>
<div class="math notranslate nohighlight" id="equation-steepest-descent">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-steepest-descent" title="Permalink to this equation">¶</a></span>\[u_{k+1} = \left(I - \lambda J'\right)(u_k),\]</div>
<p>with <span class="math notranslate nohighlight">\(\lambda \in (0,(2L)^{-1})\)</span> produces iterates <span class="math notranslate nohighlight">\(u_k\)</span> for which</p>
<div class="math notranslate nohighlight">
\[\min_{k\in \{0,1,\ldots, n-1\}} \|J'(u_k)\|_2^2 \leq \frac{J(u_0) - J_*}{C n},\]</div>
<p>with <span class="math notranslate nohighlight">\(C = \lambda \left( 1 - \textstyle{\frac{\lambda L}{2}}\right)\)</span> and <span class="math notranslate nohighlight">\(J_* = \min_u J(u)\)</span>. This implies that <span class="math notranslate nohighlight">\(\|J'(u_k)\|_2 \rightarrow 0\)</span> as <span class="math notranslate nohighlight">\(k\rightarrow \infty\)</span>. To guarantee <span class="math notranslate nohighlight">\(\min_{k\in \{0,1,\ldots, n-1\}} \|J'(u_k)\|_2 \leq \epsilon\)</span> we thus need <span class="math notranslate nohighlight">\(\mathcal{O}(1/\sqrt{\epsilon})\)</span> iterations.</p>
</div>
<div class="dropdown important admonition">
<p class="admonition-title">Proof</p>
<p>Start from a Taylor expansion:</p>
<div class="math notranslate nohighlight">
\[J(u_{k+1}) = J(u_k) + J'(u_k)(u_{k+1} - u_k) + \textstyle{\frac{1}{2}}(u_{k+1} - u_k)^T J''(\eta_k)(u_{k+1} - u_k).\]</div>
<p>Now bound the last term using the fact that <span class="math notranslate nohighlight">\(J''(u) \preceq L\cdot I\)</span> and plug in <span class="math notranslate nohighlight">\(u_{k+1} - u_k = -\lambda J'(u_k)\)</span> to get</p>
<div class="math notranslate nohighlight">
\[J(u_{k+1}) - J(u_k) \leq \lambda \left( \textstyle{\frac{\lambda L}{2}} - 1\right) \|J'(u_k)\|_2^2.\]</div>
<p>We conclude that for <span class="math notranslate nohighlight">\(0 &lt; \lambda &lt; \textstyle{\frac{1}{2L}}\)</span> we have that <span class="math notranslate nohighlight">\(J(u_{k+1}) &lt; J(u_k)\)</span> unless <span class="math notranslate nohighlight">\(\|J'(u_k)\|_2 = 0\)</span>, in which case <span class="math notranslate nohighlight">\(u_k\)</span> is a stationary point. Now, sum over <span class="math notranslate nohighlight">\(k\)</span> and re-organise to get</p>
<div class="math notranslate nohighlight">
\[\sum_{k=0}^n \|J'(u_k)\|_2^2 \leq \frac{J(u_0) - J(u_n)}{C},\]</div>
<p>with <span class="math notranslate nohighlight">\(C = \lambda \left( 1 - \textstyle{\frac{\lambda L}{2}}\right)\)</span>. Since <span class="math notranslate nohighlight">\(J_* \leq J(u_n)\)</span> we obtain the desired result.</p>
</div>
<p>Stronger statements about the <em>rate</em> of convergence can be made by making additional assumptions on <span class="math notranslate nohighlight">\(J\)</span> (such as (strong) convexity), but this is left as an exercise.</p>
</div>
<div class="section" id="linesearch">
<h3><span class="section-number">6.1.2. </span>Linesearch<a class="headerlink" href="#linesearch" title="Permalink to this headline">¶</a></h3>
<p>While the previous results are nice in theory, we usually do not have access to the Lipschitz constant <span class="math notranslate nohighlight">\(L\)</span>. Moreover, the global bound on the stepsize provided by the Lipschitz constant may be pessimistic for a particular starting point. This could lead us to pick a very small stepsize, yielding slow convergence in practice. A popular way of choosing a stepsize adaptively is a <em>linesearch</em> strategy. To introduce these, we slightly broaden the scope and consider the iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = u_k + \lambda_k d_k,\]</div>
<p>where <span class="math notranslate nohighlight">\(d_k\)</span> is a <em>descent direction</em> satisfying <span class="math notranslate nohighlight">\(\langle d_k, J'(u_k)\rangle &lt; 0\)</span>. Obviously, <span class="math notranslate nohighlight">\(d_k = - J'(u_k)\)</span> is a descent direction, but other choices may be beneficial in practice. In particular, we can choose <span class="math notranslate nohighlight">\(d_k = -B J'(u_k)\)</span> for any positive-definite matrix <span class="math notranslate nohighlight">\(B\)</span> to obtain a descent direction. How to choose such a matrix will be discussed in the next section.</p>
<p>Two important linesearch methods are discussed below.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Backtracking linesearch</em></p>
<p>In order to ensure sufficient progress of the iterations, we can choose a steplength that guarantees sufficient descent:</p>
<div class="math notranslate nohighlight" id="equation-wolfe1">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-wolfe1" title="Permalink to this equation">¶</a></span>\[J(u_k + \lambda d_k) \leq J(u_k) + c_1 \lambda \langle d_k, J'(u_k)\rangle,\]</div>
<p>with <span class="math notranslate nohighlight">\(c_1 \in (0,1)\)</span> a small constant (typically <span class="math notranslate nohighlight">\(c_1 = 10^{-4}\)</span>). Existence of a <span class="math notranslate nohighlight">\(\lambda\)</span> satisfying these conditions is guaranteed by the regularity of <span class="math notranslate nohighlight">\(J\)</span>. We can find a suitable <span class="math notranslate nohighlight">\(\lambda\)</span> by <em>backtracking</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">J</span><span class="p">,</span><span class="n">Jp</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">lmbda</span><span class="p">,</span><span class="n">rho</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">c1</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Backtracking linesearch to find a stepsize satisfying J(u + lmbda*d) &lt;= J(u) + lmbda*c1*J(u)^Td</span>

<span class="sd">Input:</span>
<span class="sd">  J  - Function object returning the value of J at a given input vector</span>
<span class="sd">  Jp - Function object returning the gradient of J at a given input vector</span>
<span class="sd">  u  - current iterate as array of length n</span>
<span class="sd">  d  - descent direction as array of length n</span>
<span class="sd">  lmbda - initial stepsize</span>
<span class="sd">  rho,c1 - backtracking parameters, default (0.5,1e-4)</span>

<span class="sd">Output:</span>
<span class="sd">  lmbda - stepsize satisfying the sufficient decrease condition</span>
<span class="sd">&quot;&quot;&quot;</span>
  <span class="k">while</span> <span class="n">J</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">lmbda</span><span class="o">*</span><span class="n">d</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">J</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span><span class="o">*</span><span class="n">lmbda</span><span class="o">*</span><span class="n">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">lmbda</span> <span class="o">*=</span> <span class="n">rho</span>
  <span class="k">return</span> <span class="n">lmbda</span>
</pre></div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Wolfe linesearch</em></p>
<p>A possible disadvantage of the backtracking linesearch introduced earlier is that it may end up choosing very small stepsizes. To obtain a stepsize that yields a new iterate at which the slope of <span class="math notranslate nohighlight">\(J\)</span> is not too large, we introduce the following condition</p>
<div class="math notranslate nohighlight" id="equation-wolfe2">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-wolfe2" title="Permalink to this equation">¶</a></span>\[|\langle J'(u_k + \lambda d_k), d_k\rangle| \leq c_2 |\langle J'(u_k), d_k\rangle|,\]</div>
<p>where <span class="math notranslate nohighlight">\(c_2\)</span> is a small constant satisfying <span class="math notranslate nohighlight">\(0 &lt; c_1 &lt; c_2 &lt; 1\)</span>. The conditions <a class="reference internal" href="#equation-wolfe1">(6.2)</a> and <a class="reference internal" href="#equation-wolfe2">(6.3)</a>, are referred to as the <em>strong Wolfe conditions</em>. Existence of a stepsize satisfying these conditions is again guaranteed by the regularity of <span class="math notranslate nohighlight">\(J\)</span> (cf. <a class="bibtex reference internal" href="#nocedal2006numerical" id="id2">[7]</a>, lemma 3.1). Finding such a <span class="math notranslate nohighlight">\(\lambda\)</span> is a little more involved than the backtracking procedure outlined above (cf. <a class="bibtex reference internal" href="#nocedal2006numerical" id="id3">[7]</a>, algorithm 3.5). Luckily, the <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> library provides an implementation of this algorithm (cf. <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html"><code class="docutils literal notranslate"><span class="pre">scipy.optimize.line_search</span></code></a>)</p>
</div>
</div>
<div class="section" id="second-order-methods">
<h3><span class="section-number">6.1.3. </span>Second order methods<a class="headerlink" href="#second-order-methods" title="Permalink to this headline">¶</a></h3>
<p>A well-known method for rootfinding is <em>Newton’s method</em>, which finds a root for which <span class="math notranslate nohighlight">\(J'(u) = 0\)</span> via the fixed point iteration</p>
<div class="math notranslate nohighlight" id="equation-newton">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-newton" title="Permalink to this equation">¶</a></span>\[u_{k+1} = u_k - J''(u_k)^{-1}J'(u_k).\]</div>
<p>We can interpret this method as finding the new iterate <span class="math notranslate nohighlight">\(u_{k+1}\)</span> as the (unique) minimiser of the quadratic approximation of <span class="math notranslate nohighlight">\(J\)</span> around <span class="math notranslate nohighlight">\(u_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[J(u) \approx J(u_k) + J'(u_k)(u - u_k) + \textstyle{\frac{1}{2}}\langle u-u_k, J''(u_k)(u-u_k)\rangle.\]</div>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Convergence of Newton’s method</em></p>
<p>Let <span class="math notranslate nohighlight">\(J\)</span> be a smooth functional and <span class="math notranslate nohighlight">\(u_*\)</span> be a (local) minimiser. For any <span class="math notranslate nohighlight">\(u_0\)</span> sufficiently close to <span class="math notranslate nohighlight">\(u_*\)</span>, the iteration <a class="reference internal" href="#equation-newton">(6.4)</a> converges quadratically to <span class="math notranslate nohighlight">\(u_*\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\|u_{k+1} - u_*\|_2 \leq M \|u_k - u_*\|_2^2,\]</div>
<p>with <span class="math notranslate nohighlight">\(M = 2\|J'''(u_*)\|_2 \|J''(u_*)^{-1}\|_2\)</span>.</p>
</div>
<div class="important dropdown admonition">
<p class="admonition-title">Proof</p>
<p>See <a class="bibtex reference internal" href="#nocedal2006numerical" id="id4">[7]</a>, Thm. 3.5.</p>
</div>
<p>In practice, the Hessian may not be invertible everywhere and we may not have an initial iterate sufficiently close to a minimiser to ensure convergence. Practical applications therefore include a linesearch and a safeguard against non-invertible Hessians.</p>
<hr class="docutils" />
<p>In some applications, it may be difficult to compute and invert the Hessian. This problem is addressed by so-called <em>quasi-Newton</em> methods which approximate the Hessian. The basis for such approximations is the <em>secant relation</em></p>
<div class="math notranslate nohighlight">
\[H_k (u_{k+1} - u_k) = (J'(u_{k+1}) - J'(u_k)),\]</div>
<p>which is satisfied by the true Hessian <span class="math notranslate nohighlight">\(J''\)</span> at a point <span class="math notranslate nohighlight">\(\eta_k = u_k + t(u_{k+1} - u_k)\)</span> for some <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>. Obviously, we cannot hope to solve for <span class="math notranslate nohighlight">\(H_k \in \mathbb{R}^{n\times n}\)</span> from just these <span class="math notranslate nohighlight">\(n\)</span> equations. We can, however, impose some structural assumptions on the Hessian.
Assuming a simple diagonal structure <span class="math notranslate nohighlight">\(H_k = h_k I\)</span> yields <span class="math notranslate nohighlight">\(h_k = \langle J'(u_{k+1}) - J'(u_k), u_{k+1} - u_k\rangle/\|u_{k+1} - u_k\|_2^2\)</span>. In fact, even gradient-descent can be interpreted in this manner by approximating <span class="math notranslate nohighlight">\(J''(u_k) \approx L \cdot I\)</span>.</p>
<hr class="docutils" />
<p>An often-used approximation is the <em>Broyden-Fletcher-Goldfarb-Shannon (BFGS)</em> approximation, which keeps track of the steps <span class="math notranslate nohighlight">\(s_k = u_{k+1} - u_k\)</span>
and gradients <span class="math notranslate nohighlight">\(y_k = J'(u_{k+1}) - J'(u_k)\)</span> to recursively construct an approximation of the <em>inverse</em> of the Hessian as</p>
<div class="math notranslate nohighlight">
\[B_{k+1} = \left(I - \rho_k s_k y_k^T\right)B_k\left(I - \rho_k y_k s_k^T\right) + \rho_k s_ks_k^T,\]</div>
<p>with <span class="math notranslate nohighlight">\(\rho_k = (\langle s_k, y_k\rangle)^{-1}\)</span> and <span class="math notranslate nohighlight">\(B_0\)</span> choses appropriately (e.g., <span class="math notranslate nohighlight">\(B_0 = L^{-1} \cdot I\)</span>). It can be shown that this approximation is sufficiently accurate to yield <em>superlinear</em> convergence when using a Wolfe linesearch.</p>
<hr class="docutils" />
<p>The are many practical aspects to implementing such methods. For example, what do we do when the approximated Hessian becomes (almost) singular? Discussing these issues is beyond the scope of these lecture notes and we refer to <a class="bibtex reference internal" href="#nocedal2006numerical" id="id5">[7]</a>, chapter 6 for more details. The <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> library provides an implementation of <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html">various optimisation methods</a>.</p>
</div>
</div>
<div class="section" id="convex-optimisation">
<h2><span class="section-number">6.2. </span>Convex optimisation<a class="headerlink" href="#convex-optimisation" title="Permalink to this headline">¶</a></h2>
<p>In this section, we consider finding a minimiser of a <em>convex</em> functional <span class="math notranslate nohighlight">\(J : \mathbb{R}^n \rightarrow \mathbb{R}_{\infty}\)</span>. Note that we allow the functionals to take values on the extended real line. We accordingly define the domain of <span class="math notranslate nohighlight">\(J\)</span> as <span class="math notranslate nohighlight">\(\text{dom}(J) = \{u \in \mathbb{R}^n \, | \, J(u) &lt; \infty\}\)</span>.</p>
<p>To deal with convex functionals that are not smooth, we first generalise the notion of a derivative.</p>
<div class="important admonition">
<p class="admonition-title">Definition: subgradient</p>
<p>Given a convex functional <span class="math notranslate nohighlight">\(J\)</span>, we call <span class="math notranslate nohighlight">\(g \in \mathbb{R}^n\)</span> a subgradient of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(u\)</span> if</p>
<div class="math notranslate nohighlight">
\[J(v) \geq J(u) + \langle g, v - u\rangle \quad \forall v \in \mathbb{R}^n.\]</div>
<p>This definition is reminiscent of the Taylor expansion and we can indeed easily check that it holds for convex smooth functionals for <span class="math notranslate nohighlight">\(g = J'(u)\)</span>. For non-smooth functionals there may be multiple vectors <span class="math notranslate nohighlight">\(g\)</span> satisfying the inequality. We call the set of all such vectors the <em>subdifferential</em> which we will denote as <span class="math notranslate nohighlight">\(\partial J(u)\)</span>. We will generally denote an arbritary element of <span class="math notranslate nohighlight">\(\partial J(u)\)</span> by <span class="math notranslate nohighlight">\(J'(u)\)</span>.</p>
</div>
<div class="admonition-example-subdifferentials-of-some-functions admonition">
<p class="admonition-title">Example: Subdifferentials of some functions</p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J_1(u) = |u|\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(J_2(u) = \delta_{[0,1]}(u)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(J_3(u) = \max\{u,0\}\)</span>.</p></li>
</ul>
<p>All these functions are convex and exhibit a discontinuity in the derivative at <span class="math notranslate nohighlight">\(u = 0\)</span>. The subdifferentials at <span class="math notranslate nohighlight">\(u=0\)</span> are given by</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\partial J_1(u) = [-1,1]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\partial J_2(u) = (-\infty,0]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\partial J_3(u) = [0,1]\)</span></p></li>
</ul>
<div class="figure align-default" id="convex-examples" style="width: 500px">
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_1_0.png" src="_images/numerical_optimisation_1_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Examples of several convex functions and an element of their subdifferential at <span class="math notranslate nohighlight">\(u=0\)</span>.</span><a class="headerlink" href="#convex-examples" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>

<span class="c1">#</span>
<span class="n">J1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
<span class="n">J2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="n">u</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">],[</span><span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mf">1e6</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mf">1e6</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">J3</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="n">u</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">],[</span><span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">u</span><span class="p">])</span>

<span class="c1">#</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">J1</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">.</span><span class="mi">1</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">J2</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">J3</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">.</span><span class="mi">9</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;convex_examples&quot;</span><span class="p">,</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_1_0.png" src="_images/numerical_optimisation_1_0.png" />
<img alt="_images/numerical_optimisation_1_1.png" src="_images/numerical_optimisation_1_1.png" />
</div>
</div>
<p>Some useful calculus rules for subgradients are listed below.</p>
<div class="important admonition">
<p class="admonition-title">Theorem: Computing subgradients</p>
<p>Let <span class="math notranslate nohighlight">\(J_i:\mathbb{R}^n \rightarrow \mathbb{R}_{\infty}\)</span> be proper convex functionals and let <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times n}\)</span>, <span class="math notranslate nohighlight">\(b \in \mathbb{R}^n\)</span>. We then have the following usefull rules</p>
<ul class="simple">
<li><p><em>Summation:</em> Let <span class="math notranslate nohighlight">\(J = J_1 + J_2\)</span>, then <span class="math notranslate nohighlight">\(J_1'(u) + J_2'(u) \in \partial J(u)\)</span> for <span class="math notranslate nohighlight">\(u\)</span> in the interior of <span class="math notranslate nohighlight">\(\text{dom}(J)\)</span>.</p></li>
<li><p><em>Affine transformation:</em> Let <span class="math notranslate nohighlight">\(J(u) = J_1(Au + b)\)</span>, then <span class="math notranslate nohighlight">\(A^T J_1'(Au + b) \in \partial J\)</span> for <span class="math notranslate nohighlight">\(u, Au + b\)</span> in the interior of <span class="math notranslate nohighlight">\(\text{dom}(J)\)</span>.</p></li>
</ul>
<p>An overview of other useful relations can be found in e.g., <a class="bibtex reference internal" href="#beck2017" id="id6">[3]</a> section 3.8.</p>
</div>
<hr class="docutils" />
<p>With this we can now formulate optimality conditions for convex optimisation.</p>
<div class="important admonition">
<p class="admonition-title">Definition: Optimality conditions for convex optimisation</p>
<p>Let <span class="math notranslate nohighlight">\(J:\mathbb{R}^n \rightarrow \mathbb{R}_{\infty}\)</span> be a proper convex functional. A point <span class="math notranslate nohighlight">\(u_* \in \mathbb{R}^n\)</span> is a minimiser iff</p>
<div class="math notranslate nohighlight">
\[0 \in J'(u_*).\]</div>
</div>
<div class="admonition-example-computing-the-median admonition">
<p class="admonition-title">Example: <em>Computing the median</em></p>
<p>The median <span class="math notranslate nohighlight">\(u\)</span> of a set of numbers <span class="math notranslate nohighlight">\((f_1, f_2, \ldots, f_n)\)</span> is a minimiser of</p>
<div class="math notranslate nohighlight">
\[J(u) = \sum_{i=1}^n |u - f_i|.\]</div>
<p>Introducing <span class="math notranslate nohighlight">\(J_i = |u - f_i|\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}J_i'(u) = \begin{cases} -1 &amp; u &lt; f_i \\ [-1,1] &amp; u = f_i \\ 1 &amp; u &gt; f_i\end{cases},\end{split}\]</div>
<p>with which we can compute <span class="math notranslate nohighlight">\(J'(u)\)</span> using the sum-rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}J'(u) = \begin{cases} -n &amp; u &lt; f_1 \\ 2i - n &amp; u \in (a_i,a_{i+1})\\ 2i-1-n+[-1,1] &amp; u = f_i\\n &amp; f&gt; f_n\end{cases}.\end{split}\]</div>
<p>To find a <span class="math notranslate nohighlight">\(u\)</span> for which <span class="math notranslate nohighlight">\(0\in J'(u)\)</span> we need to consider the middle two cases. If <span class="math notranslate nohighlight">\(n\)</span> is even, we can find an <span class="math notranslate nohighlight">\(i\)</span> such that <span class="math notranslate nohighlight">\(2i = n\)</span> and get that for all <span class="math notranslate nohighlight">\(u \in [f_{n/2},f_{n/2+1}]\)</span> we have <span class="math notranslate nohighlight">\(0 \in J'(u)\)</span>.
When <span class="math notranslate nohighlight">\(n\)</span> is odd, we have optimality only for <span class="math notranslate nohighlight">\(u = f_{(n+1)/2}\)</span>.</p>
<div class="figure align-default" id="median-example" style="width: 500px">
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_3_0.png" src="_images/numerical_optimisation_3_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Subgradient of <span class="math notranslate nohighlight">\(J\)</span> for <span class="math notranslate nohighlight">\(f=(1,2,3,4)\)</span> and <span class="math notranslate nohighlight">\(f=(1,2,3,4,5)\)</span>.</span><a class="headerlink" href="#median-example" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>

<span class="c1">#</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">f2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>

<span class="c1">#</span>
<span class="n">Jip</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span><span class="n">f</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">u</span><span class="p">,[</span><span class="n">u</span><span class="o">&lt;</span><span class="n">f</span><span class="p">,</span><span class="n">u</span><span class="o">==</span><span class="n">f</span><span class="p">,</span><span class="n">u</span><span class="o">&gt;</span><span class="n">f</span><span class="p">],[</span><span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f</span><span class="p">):</span>
  <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">Jip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">g</span>

<span class="c1">#</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f1</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$f = (1,2,3,4)$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">Jp</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">f2</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$f = (1,2,3,4,5)$&#39;</span><span class="p">)</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;median_example&quot;</span><span class="p">,</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_3_0.png" src="_images/numerical_optimisation_3_0.png" />
<img alt="_images/numerical_optimisation_3_1.png" src="_images/numerical_optimisation_3_1.png" />
</div>
</div>
<div class="section" id="subgradient-descent">
<h3><span class="section-number">6.2.1. </span>Subgradient descent<a class="headerlink" href="#subgradient-descent" title="Permalink to this headline">¶</a></h3>
<p>A natural extension of the gradient-descent method for smooth problems is the <em>subgradient descent method</em>:</p>
<div class="math notranslate nohighlight" id="equation-subgradient-descent">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-subgradient-descent" title="Permalink to this equation">¶</a></span>\[u_{k+1} = u_k - \lambda_k J'(u_k), \quad J'(u_k) \in \partial J(u_k),\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_k\)</span> denote the stepsizes.</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Convergence of subgradient descent</em></p>
<p>Let <span class="math notranslate nohighlight">\(J : \mathbb{R}^n \rightarrow \mathbb{R}\)</span> be a convex, <span class="math notranslate nohighlight">\(L-\)</span> Lipschitz-continuous function. The iteration <a class="reference internal" href="#equation-subgradient-descent">(6.5)</a> produces iterates for which</p>
<div class="math notranslate nohighlight">
\[\min_{k\in\{0,1,\ldots,n-1\}} J(u_k) - J(u_*) \leq \frac{\|u_0 - u_*\|_2^2 + L^2 \sum_{k=0}^{n-1}\lambda_k^2}{2\sum_{k=0}^{n-1}\lambda_k}.\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(J(u_k) \rightarrow J(u_*)\)</span> as <span class="math notranslate nohighlight">\(k\rightarrow \infty\)</span> when the stepsize satisfy</p>
<div class="math notranslate nohighlight">
\[\sum_{k=0}^{\infty} \lambda_k = \infty, \quad \sum_{k=0}^{\infty} \lambda_k^2 &lt; \infty.\]</div>
</div>
<div class="important dropdown admonition">
<p class="admonition-title">Proof</p>
<p>By using the definition of <span class="math notranslate nohighlight">\(u_{k+1}\)</span> and the subgradient inequality, we find</p>
<div class="math notranslate nohighlight">
\[\|u_{k+1} - u_*\|_2^2 \leq \|u_{k} - u_*\|_2^2 -2 \lambda_k \left(J_k - J_*\right) + \lambda_k^2\|J'_k\|_2^2.\]</div>
<p>Applying this inequality recursively on <span class="math notranslate nohighlight">\(u_{k}\)</span> yields</p>
<div class="math notranslate nohighlight">
\[\|u_{k+1} - u_*\|_2^2 \leq \|u_0 - u_*\|_2^2 -2\sum_{i=0}^k \lambda_i \left(J_i - J_*\right) + \sum_{i=0}^k \lambda_k \|J'_i\|_2^2.\]</div>
<p>Using Lipschitz continuity of <span class="math notranslate nohighlight">\(J\)</span>, we find <span class="math notranslate nohighlight">\(\|J'(u)\|_2 \leq L\)</span> which can be used to yield the desired result.</p>
</div>
<div class="admonition-remark-convergence-rate-for-a-fixed-stepsize admonition">
<p class="admonition-title">Remark: <em>Convergence rate for a fixed stepsize</em></p>
<p>If we choose <span class="math notranslate nohighlight">\(\lambda_k = \lambda\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[\min_{k\in\{0,1,\ldots,n-1\}} J(u_k) - J(u_*) \leq \frac{\|u_0 - u_*\|_2^2 + L^2 \lambda^2 n^2}{2\lambda n}.\]</div>
<p>we can guarantee that <span class="math notranslate nohighlight">\(\min_{k\in\{0,1,\ldots,n-1\}} J(u_k) - J(u_*) \leq \epsilon\)</span> by picking stepsize <span class="math notranslate nohighlight">\(\lambda = \epsilon/L^2\)</span> and doing <span class="math notranslate nohighlight">\(n = (\|u_0-u_*\|_2L/\epsilon)^2\)</span> iterations. This seems comparable to the rate we derived for gradient descent previously. However, for <em>smooth</em> convex functions we derive a stronger result that gradient-descent requires only <span class="math notranslate nohighlight">\(\mathcal{O}(1/\epsilon)\)</span> iterations. For smooth <em>strongly</em> convex functionals we can strengthen the result even further and show that we only need <span class="math notranslate nohighlight">\(\mathcal{O}(\log 1/\epsilon)\)</span> iterations. The proofs are left as an exercise.</p>
</div>
</div>
<div class="section" id="proximal-gradient-methods">
<h3><span class="section-number">6.2.2. </span>Proximal gradient methods<a class="headerlink" href="#proximal-gradient-methods" title="Permalink to this headline">¶</a></h3>
<p>While the subgradient descent method is easily implemented, it does not fully exploit the structure of the objective. In particular, we can often split the objective in a <em>smooth</em> and a <em>convex</em> part. For the discussion we will assume for the moment that</p>
<div class="math notranslate nohighlight">
\[J(u) = D(u) + R(u),\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is smooth and <span class="math notranslate nohighlight">\(R\)</span> is convex. We are then looking for a point <span class="math notranslate nohighlight">\(u_*\)</span> for which</p>
<div class="math notranslate nohighlight" id="equation-diff-inclusion">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-diff-inclusion" title="Permalink to this equation">¶</a></span>\[D'(u_*) \in -\partial R(u_*).\]</div>
<p>Finding such a point can be done (again!) by a fixed-point iteration</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I + \lambda \partial R\right)^{-1}\left(I - \lambda D'\right)(u_k),\]</div>
<p>where <span class="math notranslate nohighlight">\(u = \left(I + \lambda \partial R\right)^{-1}(v)\)</span> yields a point <span class="math notranslate nohighlight">\(u\)</span> for which <span class="math notranslate nohighlight">\(\lambda^{-1}(v - u) \in \partial R(u)\)</span>. We can easily show that a fixed point of this iteration indeed solves the differential inclusion problem <a class="reference internal" href="#equation-diff-inclusion">(6.6)</a>. Assuming a fixed point <span class="math notranslate nohighlight">\(u_*\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[u_{*} = \left(I + \lambda \partial R\right)^{-1}\left(I - \lambda D'\right)(u_*),\]</div>
<p>using the definition of <span class="math notranslate nohighlight">\(\left(I + \lambda \partial R\right)^{-1}\)</span> this yields</p>
<div class="math notranslate nohighlight">
\[\lambda^{-1}\left(u_* - \lambda D'(u_*) - u_*\right) \in \partial R(u_*),\]</div>
<p>which indeed confirms that <span class="math notranslate nohighlight">\(-D'(u_*) \in \partial R(u_*)\)</span>.</p>
<hr class="docutils" />
<div class="important admonition">
<p class="admonition-title">Definition: Proximal operator</p>
<p>The operator <span class="math notranslate nohighlight">\(\left(I + \lambda \partial R\right)^{-1}\)</span> is called the <em>proximal operator</em> of <span class="math notranslate nohighlight">\(\lambda R\)</span>, whose action on input <span class="math notranslate nohighlight">\(v\)</span> is implicitly defined as solving</p>
<div class="math notranslate nohighlight">
\[\min_u \textstyle{\frac{1}{2}} \|u - v\|_2^2 + \lambda R(u).\]</div>
<p>We usually denote this operator by <span class="math notranslate nohighlight">\(\text{prox}_{\lambda R}(v)\)</span>.</p>
</div>
<p>With this, the proximal gradient method for solving <a class="reference internal" href="#equation-diff-inclusion">(6.6)</a> is then denoted as</p>
<div class="math notranslate nohighlight" id="equation-proximal-gradient">
<span class="eqno">(6.7)<a class="headerlink" href="#equation-proximal-gradient" title="Permalink to this equation">¶</a></span>\[u_{k+1} = \text{prox}_{\lambda R}\left(u_k - \lambda D'(u_k)\right).\]</div>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Convergence of the proximal point iteration</em></p>
<p>Let <span class="math notranslate nohighlight">\(J = D + R\)</span> be a functional with <span class="math notranslate nohighlight">\(D\)</span> smooth and <span class="math notranslate nohighlight">\(R\)</span> convex. Denote the Lipschitz constant of <span class="math notranslate nohighlight">\(D'\)</span> by <span class="math notranslate nohighlight">\(L_D\)</span>. The iterates produced by <a class="reference internal" href="#equation-proximal-gradient">(6.7)</a> with a fixed stepsize <span class="math notranslate nohighlight">\(\lambda = 1/L_D\)</span> converge to a fixed point, <span class="math notranslate nohighlight">\(u_*\)</span>, of <a class="reference internal" href="#equation-proximal-gradient">(6.7)</a>.</p>
<p>If, in addition, <span class="math notranslate nohighlight">\(D\)</span> is convex the iterates converges sublinearly to a minimiser <span class="math notranslate nohighlight">\(u_*\)</span>:</p>
<div class="math notranslate nohighlight">
\[J(u_k) - J_* \leq \frac{L_D \|u_* - u_0\|_2^2}{2k}.\]</div>
<p>If <span class="math notranslate nohighlight">\(D\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>-strongly convex, the iteration converges linearly to a minimiser <span class="math notranslate nohighlight">\(u_*\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|u_{k+1} - u_*\|_2^2 \leq \left(1 - \mu/L_D\right) \|u_{k} - u_*\|_2^2.\]</div>
</div>
<div class="dropdown important admonition">
<p class="admonition-title">Proof</p>
<p>We refer to <a class="bibtex reference internal" href="#beck2017" id="id7">[3]</a> Thms. 10.15, 10.21, and 10.29 or more details.</p>
</div>
<hr class="docutils" />
<p>When compared to the subgradient method, we may expect better performance from the proximal gradient method when <span class="math notranslate nohighlight">\(D\)</span> is strongly convex and <span class="math notranslate nohighlight">\(R\)</span> is convex. Even if <span class="math notranslate nohighlight">\(J\)</span> is smooth, the proximal gradient method may be favourable as the convergence constants depend on the Lipschitz constant of <span class="math notranslate nohighlight">\(D\)</span> only; not <span class="math notranslate nohighlight">\(J\)</span>. All this comes at the cost of solving a minimisation problem at each iteration, so these methods are usually only applied when a closed-form expression for the proximal operator exists.</p>
<div class="admonition-example-one-norm admonition">
<p class="admonition-title">Example: <em>one-norm</em></p>
<p>The proximal operator for the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm solves</p>
<div class="math notranslate nohighlight">
\[\min_u \textstyle{\frac{1}{2}}\|u - v\|_2 + \lambda \|u\|_1.\]</div>
<p>The solution obeys <span class="math notranslate nohighlight">\(u - v \in -\partial \lambda \|u\|_1\)</span>, which yields</p>
<div class="math notranslate nohighlight">
\[\begin{split}u_i - v_i \in \begin{cases} \{-\lambda\} &amp; u_i &gt; 0 \\ [-\lambda,\lambda] &amp; u_i = 0 \\ \{\lambda\} &amp; u_i &lt; 0\end{cases}\end{split}\]</div>
<p>This condition is fulfulled by setting</p>
<div class="math notranslate nohighlight">
\[\begin{split}u_i = \begin{cases}v_i - \lambda &amp; v_i &gt; \lambda \\ 0 &amp; |v_i|\leq \lambda \\ v_i + \lambda &amp; v_i &lt; -\lambda \end{cases}\end{split}\]</div>
</div>
<div class="admonition-example-box-constraints admonition">
<p class="admonition-title">Example: <em>box constraints</em></p>
<p>The Proximal operator of the indicator function of <span class="math notranslate nohighlight">\(\delta_{[a,b]^n}\)</span> solves</p>
<div class="math notranslate nohighlight">
\[\min_{[a,b]^n} \textstyle{\frac{1}{2}}\|u - v\|_2.\]</div>
<p>The solution is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}u_i = \begin{cases} a &amp; v_i &lt; a \\ v_i &amp; v_i \in [a,b] \\ b &amp; v_i &gt; b\end{cases}.\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(u\)</span> is an orthogonal projection of <span class="math notranslate nohighlight">\(v\)</span> on <span class="math notranslate nohighlight">\([a,b]^n\)</span>.</p>
</div>
</div>
<div class="section" id="splitting-methods">
<h3><span class="section-number">6.2.3. </span>Splitting methods<a class="headerlink" href="#splitting-methods" title="Permalink to this headline">¶</a></h3>
<p>The proximal point methods require that the proximal operator for <span class="math notranslate nohighlight">\(R\)</span> can be evaluated efficiently. In many practical applications this is not the cases, however. Instead, we may have a regulariser of the form <span class="math notranslate nohighlight">\(R(Au)\)</span> for some linear operator <span class="math notranslate nohighlight">\(A\)</span>. Even when <span class="math notranslate nohighlight">\(R(\cdot)\)</span> admits an efficient proximal operator <span class="math notranslate nohighlight">\(R(A\cdot)\)</span> will, in general, not. In this section we discuss a class of methods that allow us to shift the operator <span class="math notranslate nohighlight">\(A\)</span> to the other part of the objective. As a model-problem we will consider solving</p>
<div class="math notranslate nohighlight">
\[\min_{u\in \mathbb{R}^n} D(u) + R(Au),\]</div>
<p>with <span class="math notranslate nohighlight">\(D\)</span> smooth and <span class="math notranslate nohighlight">\(\mu-\)</span> strongly convex, <span class="math notranslate nohighlight">\(R(\cdot)\)</span> convex and <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m\times n}\)</span> a linear map. The basic idea is to introduce an auxiliary variable <span class="math notranslate nohighlight">\(v\)</span> and re-formulate the variational problem as</p>
<div class="math notranslate nohighlight" id="equation-splitted">
<span class="eqno">(6.8)<a class="headerlink" href="#equation-splitted" title="Permalink to this equation">¶</a></span>\[\min_{u\in \mathbb{R}^n,v\in\mathbb{R}^m} D(u) + R(v), \quad \text{s.t.} \quad Au = v.\]</div>
<p>To solve such constrained optimization problems we employ the method of Lagrange multipliers which defines the <em>Lagrangian</em></p>
<div class="math notranslate nohighlight">
\[\Lambda(u,v,\nu) = D(u) + R(v) + \langle \nu, Au - v\rangle,\]</div>
<p>where <span class="math notranslate nohighlight">\(\nu \in \mathbb{R}^m\)</span> are called the Lagrange multipliers. The solution to <a class="reference internal" href="#equation-splitted">(6.8)</a> is a saddle point of <span class="math notranslate nohighlight">\(\Lambda\)</span> and we can thus be obtained by solving</p>
<div class="math notranslate nohighlight" id="equation-saddle-point">
<span class="eqno">(6.9)<a class="headerlink" href="#equation-saddle-point" title="Permalink to this equation">¶</a></span>\[\min_{u,v} \max_{\nu} \Lambda(u,v,\nu).\]</div>
<p>The equivalence between <a class="reference internal" href="#equation-splitted">(6.8)</a> and <a class="reference internal" href="#equation-saddle-point">(6.9)</a> is established in the following theorem</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Saddle point theorem</em></p>
<p>Let <span class="math notranslate nohighlight">\((u_*,v_*)\)</span> be a solution to <a class="reference internal" href="#equation-splitted">(6.8)</a>, then there exists a <span class="math notranslate nohighlight">\(\nu^*\in\mathbb{R}^m\)</span> such that <span class="math notranslate nohighlight">\((u_*,v_*,\nu_*)\)</span> is a saddle point of <span class="math notranslate nohighlight">\(\Lambda\)</span> and vice versa.</p>
</div>
<div class="important dropdown admonition">
<p class="admonition-title">Proof</p>
<p>see <a class="reference external" href="https://sites.math.washington.edu/~burke/crs/516/notes/saddlepoints.pdf">these notes</a></p>
</div>
<p>Another important concept related to the Lagrangian is the <em>dual problem</em>.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Dual problem</em></p>
<p>The dual problem related to <a class="reference internal" href="#equation-saddle-point">(6.9)</a> is</p>
<div class="math notranslate nohighlight" id="equation-dual-saddle-point">
<span class="eqno">(6.10)<a class="headerlink" href="#equation-dual-saddle-point" title="Permalink to this equation">¶</a></span>\[\max_{\nu} \min_{u,v} \Lambda(u,v,\nu).\]</div>
</div>
<p>For convex problems, the primal and dual problems are equivalent, giving us freedom when designing algorithms.</p>
<div class="admonition-theorem-strong-duality admonition">
<p class="admonition-title">Theorem: <em>Strong duality</em></p>
<p>The primal <a class="reference internal" href="#equation-saddle-point">(6.9)</a> and dual <a class="reference internal" href="#equation-dual-saddle-point">(6.10)</a> are equivalent in the sense that</p>
<div class="math notranslate nohighlight">
\[ \min_{u,v} \max_{\nu} \Lambda(u,v,\nu) = \max_{\nu} \min_{u,v} \Lambda(u,v,\nu).\]</div>
</div>
<div class="important dropdown admonition">
<p class="admonition-title">Proof</p>
<p>see <a class="reference external" href="https://sites.math.washington.edu/~burke/crs/516/notes/saddlepoints.pdf">these notes</a></p>
</div>
<div class="admonition-example-tv-denoising admonition">
<p class="admonition-title">Example: TV-denoising</p>
<p>The TV-denoising problem can be expressed as</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbb{R}^n} \textstyle{\frac{1}{2}} \|u - f^\delta\|_2^2 + \lambda \|Du\|_1,\]</div>
<p>with <span class="math notranslate nohighlight">\(D \in \mathbb{R}^{m \times n}\)</span> a discretisation of the first derivative. We can express the corresponding dual problem as</p>
<div class="math notranslate nohighlight">
\[\max_{\nu} \min_u \textstyle{\frac{1}{2}} \|u - f^\delta\|_2^2 + \langle \nu,Du\rangle + \min_v \lambda \|v\|_1 - \langle \nu, v\rangle.\]</div>
<p>The first term is minimised by setting <span class="math notranslate nohighlight">\(u = f^\delta - D^*\nu\)</span>. The second term is a bit trickier. First, we note that <span class="math notranslate nohighlight">\(\lambda \|v\|_1 - \langle \nu, v\rangle\)</span> is not bounded from below when <span class="math notranslate nohighlight">\(\|\nu\|_{\infty} &gt; \lambda\)</span>. Furthermore, for <span class="math notranslate nohighlight">\(\|\nu\|_{\infty} \leq \lambda\)</span> it attains a minimum for <span class="math notranslate nohighlight">\(v = 0\)</span>.</p>
<p>This leads to</p>
<div class="math notranslate nohighlight">
\[\max_{\nu} -\textstyle{\frac{1}{2}} \|D^*\nu\|_2^2 + \langle D^*\nu, f^\delta\rangle - \delta_{\|\cdot\|_\infty \leq \lambda}(\nu),\]</div>
<p>which is a constrained quadratic program. Since the first part is smooth and the proximal operator for the constraint <span class="math notranslate nohighlight">\(\|\nu\|_{\infty} \leq \lambda\)</span> is easy we can employ a proximal gradient method to solve the dual problem. Having solved it, we can retrieve the primary variable via the relation <span class="math notranslate nohighlight">\(u = f^\delta - D^*\nu\)</span>.</p>
</div>
<p>The strategy illustrated in the previous approach is an example of a more general approach to solving problems of form <a class="reference internal" href="#equation-splitted">(6.8)</a>.</p>
<div class="important admonition">
<p class="admonition-title"><em>Dual-based proximal gradient</em></p>
<p>We start from the dual problem <a class="reference internal" href="#equation-dual-saddle-point">(6.10)</a>:</p>
<div class="math notranslate nohighlight">
\[\max_{\nu} \left(\min_u D(u) + \langle Au,\nu\rangle\right) + \left(\min_v R(v) - \langle \nu, v\rangle\right).\]</div>
<p>In this expression we recognise the <a class="reference external" href="https://en.wikipedia.org/wiki/Convex_conjugate"><em>convex conjugates</em></a> of <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(R\)</span>. With this, we re-write the problem as</p>
<div class="math notranslate nohighlight">
\[\min_{\nu} D^*(A^T\nu) + R^*(-\nu).\]</div>
<p>Thus, we have moved the linear map to the other side. We can now apply the proximal gradient method provided that:</p>
<ul class="simple">
<li><p>We have a closed-form expression for the convex conjugates of <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(R\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(R^*\)</span> has a proximal operator that is easily evaluated.</p></li>
</ul>
<p>For many simple functions, we do have such closed-form expressions of their convex conjugates. Moreover, to compute the proximal operator, we can use <em>Moreau’s identity:</em> <span class="math notranslate nohighlight">\(\text{prox}_{R}(u) + \text{prox}_{R^*}(u) = u\)</span>.</p>
</div>
<hr class="docutils" />
<p>It may not always be feasible to formulate the dual problem explicitly as in the previous example. In such cases we would rather solve <a class="reference internal" href="#equation-dual-saddle-point">(6.10)</a> directly. A popular way of doing this is the <em>Alternating Direction of Multipliers Method</em>.</p>
<div class="important admonition">
<p class="admonition-title"><em>Alternating Direction of Multipliers Method (ADMM)</em></p>
<p>We augment the Lagrangian by adding a quadratic term:</p>
<div class="math notranslate nohighlight">
\[\Lambda_{\rho}(u,v,\nu) = D(u) + R(v) + \langle\nu,Au - v\rangle + \textstyle{\frac{\rho}{2}} \|Au - v\|_2^2.\]</div>
<p>We then find the solution by updating the variables in an alternating fashion</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \text{arg}\min_{u} \Lambda_{\rho}(u,v_k,\nu_k),\]</div>
<div class="math notranslate nohighlight">
\[v_{k+1} = \text{arg}\min_{v} \Lambda_{\rho}(u_{k+1},v,\nu_k),\]</div>
<div class="math notranslate nohighlight">
\[\nu_{k+1} = \nu_k + \rho(Au_{k+1} - v_{k+1}).\]</div>
<p>Efficient implementations of this method rely on the proximal operators of <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(R\)</span>.</p>
</div>
<div class="admonition-example-tv-denoising admonition">
<p class="admonition-title">Example:<em>TV-denoising</em></p>
<p>Consider the TV-denoising problem from the previous example.</p>
<p>The ADMM method find a solution via</p>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I + \rho D^*\!D\right)^{-1}\left(f^\delta + D^*(\rho v_k - \nu_k)\right).\]</div>
<div class="math notranslate nohighlight">
\[v_{k+1} = \text{prox}_{(\lambda/\rho)\|\cdot\|_1}\left(Du_{k+1} + \rho^{-1}\nu_k\right).\]</div>
<div class="math notranslate nohighlight">
\[\nu_{k+1}= \nu_k + \rho \left(Du_{k+1} - v_{k+1}\right).\]</div>
</div>
<hr class="docutils" />
<p>We cannot do justice to the breadth and depth of the topics smooth and convex optimisation in one chapter. Rather, we hope that this chapter serves as a starting point for further study in one of these areas for some, and provides useful recipes for others.</p>
</div>
</div>
<div class="section" id="references">
<h2><span class="section-number">6.3. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-numerical_optimisation-0"><dl class="citation">
<dt class="bibtex label" id="beck2017"><span class="brackets">3</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Amir Beck. <em>First-Order Methods in Optimization</em>. Society for Industrial and Applied Mathematics, 2017.</p>
</dd>
<dt class="bibtex label" id="nocedal2006numerical"><span class="brackets">7</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>,<a href="#id4">4</a>,<a href="#id5">5</a>)</span></dt>
<dd><p>Jorge Nocedal and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.</p>
</dd>
</dl>
</p>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">6.4. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="steepest-descent-for-strongly-convex-functionals">
<h3><span class="section-number">6.4.1. </span>Steepest descent for strongly convex functionals<a class="headerlink" href="#steepest-descent-for-strongly-convex-functionals" title="Permalink to this headline">¶</a></h3>
<p>Consider the following fixed point iteration for minimizing a given function <span class="math notranslate nohighlight">\(J : \mathbb{R}^n \rightarrow \mathbb{R}\)</span></p>
<div class="math notranslate nohighlight">
\[
u^{(k+1)} = u^{(k)} - \alpha J'(u^{(k)}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(J\)</span> is twice continuously differentiable and strictly convex:</p>
<div class="math notranslate nohighlight">
\[
\mu I \preceq J''(u) \preceq L I,
\]</div>
<p>with <span class="math notranslate nohighlight">\(0 &lt; \mu &lt; L &lt; \infty\)</span>.</p>
<ul class="simple">
<li><p>Show that the fixed point iteration converges linearly, i.e.,
<span class="math notranslate nohighlight">\(\|u^{(k+1)} - u^*\| \leq \rho \|u^{(k)} - u*\|\)</span> with <span class="math notranslate nohighlight">\(\rho &lt; 1\)</span>, for <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 2/L\)</span>.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Rate_of_convergence#Basic_definition">Linear convergence</a> implies that <span class="math notranslate nohighlight">\(\exists 0 &lt; \rho &lt; 1\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\|u^{(k+1)} - u^*\| \leq \rho \|u^{(k)} - u*\|,\]</div>
<p>where <span class="math notranslate nohighlight">\(u^*\)</span>. To show this we start from the iteration and substract the fixed-point and use that <span class="math notranslate nohighlight">\(J'(u^*) = 0\)</span> to get</p>
<div class="math notranslate nohighlight">
\[(u^{(k+1)} - u^*) = (u^{(k)} - u^*) - \alpha (J'(u^{(k)}) - J'(u^*)).\]</div>
<p>Next use Taylor to express</p>
<div class="math notranslate nohighlight">
\[J'(u^{(k)}) - J'(u^*) = J''(\eta^{(k)}) (u^{(k)} - u^*),\]</div>
<p>with <span class="math notranslate nohighlight">\(\eta^{(k)} = t u^{(k)} + (1-t)u^*\)</span> for some <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>. We then get</p>
<div class="math notranslate nohighlight">
\[\|u^{(k+1)} - u^*\|_2 \leq \|I - \alpha J''(\eta^{(k)})\|_2 \|u^{(k)} - u^*\|_2.\]</div>
<p>For linear convergence we need <span class="math notranslate nohighlight">\(\|I - \alpha J''(\eta^{(k)})\|_2 &lt; 1\)</span>. We use that <span class="math notranslate nohighlight">\(\|A\|_2 = \sigma_{\max}(A)\)</span>. (cf. <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_norm#Special_cases">Matrix norms</a>)
Since the eigenvalues of <span class="math notranslate nohighlight">\(\J''\)</span> are bounded by <span class="math notranslate nohighlight">\(L\)</span> we need <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 2/L\)</span> to ensure this.</p>
</div>
<ul class="simple">
<li><p>Determine the value of <span class="math notranslate nohighlight">\(\alpha\)</span> for which the iteration converges fastest.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<p>The smaller the bound on the constant <span class="math notranslate nohighlight">\(\rho\)</span>, the faster the convergence. We have</p>
<div class="math notranslate nohighlight">
\[\|I - \alpha J''(\eta^{(k)})\|_2 = \ \max (|1 - \alpha \mu|, |1 - \alpha L|).\]</div>
<p>We obtain the smalles possible value by making both terms equal, for which we need</p>
<div class="math notranslate nohighlight">
\[(1 - \alpha \mu) = -(1 - \alpha L),\]</div>
<p>this gives us an optimal value of <span class="math notranslate nohighlight">\(\alpha = 2/(\mu + L)\)</span>.</p>
</div>
</div>
<div class="section" id="steepest-descent-for-convex-functions">
<h3><span class="section-number">6.4.2. </span>Steepest descent for convex functions<a class="headerlink" href="#steepest-descent-for-convex-functions" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(J : \mathbb{R}^n\)</span> be convex and Lipschitz-smooth. Show that the basic steepest-descent iteration with stepsize <span class="math notranslate nohighlight">\(\lambda = 1/L\)</span> produces iterates for which</p>
<div class="math notranslate nohighlight">
\[J(u_k) - J(u_*) \leq \frac{\|u_0 - u_*\|}{2}.\]</div>
<p>The key is to use that</p>
<div class="math notranslate nohighlight">
\[J(v) \leq J(u) + \langle J'(u), v - u\rangle + \textstyle{\frac{1}{2}}\|u - v\|_2^2.\]</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Start from the inequality and set <span class="math notranslate nohighlight">\(v = u_{k+1} = u_k - \lambda J'(u_k)\)</span> and <span class="math notranslate nohighlight">\(u = u_k\)</span>, yielding</p>
<div class="math notranslate nohighlight">
\[J(u_{k+1}) \leq J(u_k) - \frac{1}{2L}\|J'(u_k)\|_2^2.\]</div>
<p>Using convexity of <span class="math notranslate nohighlight">\(J\)</span> we find</p>
<div class="math notranslate nohighlight">
\[J_{k+1} - J_* \leq \frac{1}{2L}\left(\|u_k - u_*\|_2^2 - \|u_{k+1} - u_*\|_2^2\right).\]</div>
<p>Using a telescoping sum and the fact that <span class="math notranslate nohighlight">\(J(u_{k+1}) \leq J(u_k)\)</span> yields the desired result.</p>
</div>
</div>
<div class="section" id="rosenbrock">
<h3><span class="section-number">6.4.3. </span>Rosenbrock<a class="headerlink" href="#rosenbrock" title="Permalink to this headline">¶</a></h3>
<p>We are going to test various optimization methods on the Rosenbrock function</p>
<div class="math notranslate nohighlight">
\[
f(x,y) = (a - x)^2 + b(y - x^2)^2,
\]</div>
<p>with <span class="math notranslate nohighlight">\(a = 1\)</span> and <span class="math notranslate nohighlight">\(b = 100\)</span>. The function has a global minimum at <span class="math notranslate nohighlight">\((a, a^2)\)</span>.</p>
<ul class="simple">
<li><p>Write a function to compute the Rosenbrock function, its gradient and the Hessian for given input <span class="math notranslate nohighlight">\((x,y)\)</span>. Visualize the function on <span class="math notranslate nohighlight">\([-3,3]^2\)</span> and indicate the neighborhood around the minimum where <span class="math notranslate nohighlight">\(f\)</span> is convex.</p></li>
<li><p>Implement the method from exercise 1 and test convergence from various initial points. Does the method always convergce? How small do you need to pick <span class="math notranslate nohighlight">\(\alpha\)</span>? How fast?</p></li>
<li><p>Implement a linesearch strategy to ensure that <span class="math notranslate nohighlight">\(\alpha_k\)</span> satisfies the Wolfe conditions, does <span class="math notranslate nohighlight">\(\alpha\)</span> vary a lot?</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>In de code below, we show a graph of the function and determine the region of convexity by computing the eigenvalues of the Hessian (should be positive)</p></li>
<li><p>We observe linear convergence for small enough <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
<li><p>Using a linesearch we obtain faster convergence by allowing larger steps in the beginning.</p></li>
</ul>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">line_search</span>

<span class="c1"># rosenbrock function</span>
<span class="k">def</span> <span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x1</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)])</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">12</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">x1</span><span class="o">*</span><span class="n">b</span><span class="p">],[</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">H</span>

<span class="c1"># steepest descent</span>
<span class="k">def</span> <span class="nf">steep</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">niter</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">fk</span><span class="p">,</span><span class="n">gk</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gk</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># steepest descent with linesearch</span>
<span class="k">def</span> <span class="nf">steep_wolfe</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">alpha0</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">niter</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">fk</span><span class="p">,</span><span class="n">gk</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">pk</span> <span class="o">=</span> <span class="o">-</span><span class="n">alpha0</span><span class="o">*</span><span class="n">gk</span> <span class="c1">#reference stepsize</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">line_search</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">pk</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">alpha</span><span class="p">:</span> <span class="c1"># check if linesearch was successfull</span>
            <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">pk</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># if not, use regular step</span>
            <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">pk</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot of the Rosenbrock function</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">fs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">],</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">((</span><span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,</span><span class="n">fs</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7ff51fb83ed0&gt;]
</pre></div>
</div>
<img alt="_images/numerical_optimisation_7_1.png" src="_images/numerical_optimisation_7_1.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># determine region of convexity by computing eigenvalues of the Hessian</span>
<span class="n">e1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">e2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">Hs</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">((</span><span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
        <span class="n">e1</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">],</span><span class="n">e2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">Hs</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,(</span><span class="n">e1</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">e2</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">),</span><span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7ff521c47e10&gt;]
</pre></div>
</div>
<img alt="_images/numerical_optimisation_8_1.png" src="_images/numerical_optimisation_8_1.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run steepest descent</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">12122</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.99</span><span class="o">/</span><span class="n">L</span>
<span class="n">maxiter</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">steep</span><span class="p">(</span><span class="n">rosenbrock</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span><span class="n">alpha</span><span class="p">,</span><span class="n">maxiter</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxiter</span><span class="p">,</span><span class="n">maxiter</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,</span><span class="n">fs</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xs</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">k</span><span class="p">,(</span><span class="o">.</span><span class="mi">99993</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$k$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;error&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_9_0.png" src="_images/numerical_optimisation_9_0.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run steepest descent with linesearch</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">12122</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.99</span><span class="o">/</span><span class="n">L</span>
<span class="n">maxiter</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">steep_wolfe</span><span class="p">(</span><span class="n">rosenbrock</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span><span class="mf">1.99</span><span class="o">/</span><span class="n">L</span><span class="p">,</span><span class="mi">50000</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxiter</span><span class="p">,</span><span class="n">maxiter</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span><span class="n">xx2</span><span class="p">,</span><span class="n">fs</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xs</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">k</span><span class="p">,(</span><span class="o">.</span><span class="mi">99993</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="stderr docutils container">
<pre class="stderr literal-block">/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)
/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)
</pre>
</div>
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7ff52290c5d0&gt;,
 &lt;matplotlib.lines.Line2D at 0x7ff52389ee10&gt;]
</pre></div>
</div>
<img alt="_images/numerical_optimisation_10_2.png" src="_images/numerical_optimisation_10_2.png" />
</div>
</div>
</div>
<div class="section" id="subdifferentials">
<h3><span class="section-number">6.4.4. </span>Subdifferentials<a class="headerlink" href="#subdifferentials" title="Permalink to this headline">¶</a></h3>
<p>Compute the subdifferentials of the following functionals <span class="math notranslate nohighlight">\(J : \mathbb{R}^n \rightarrow \mathbb{R}_+\)</span>:</p>
<ul class="simple">
<li><p>The Euclidean norm <span class="math notranslate nohighlight">\(J(u) = \|u\|_2\)</span>.</p></li>
<li><p>The elastic net <span class="math notranslate nohighlight">\(J(u) = \alpha \|u\|_1 + \beta \|u\|_2^2\)</span></p></li>
<li><p>The weighted <span class="math notranslate nohighlight">\(\ell_1\)</span>-norm <span class="math notranslate nohighlight">\(J(u) = \|Du\|_1\)</span>, with <span class="math notranslate nohighlight">\(D \in \mathbb{R}^{m\times n}\)</span> for <span class="math notranslate nohighlight">\(m &lt; n\)</span> a full-rank matrix.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(\|u\|_2 \not=0\)</span> we have <span class="math notranslate nohighlight">\(\partial J(u) = \{u/\|u\|_2\}\)</span>. Otherwise we have <span class="math notranslate nohighlight">\(\partial J(0) = \{g \in \mathbb{R}^n | \|g\|_2 \leq 1\}\)</span>. Indeed, we can check that for all <span class="math notranslate nohighlight">\(g \in \partial J(0)\)</span> we have <span class="math notranslate nohighlight">\(\langle g,v\rangle \leq \|v\|_2\)</span> for all <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>.</p></li>
<li><p>An element <span class="math notranslate nohighlight">\(g\)</span> of <span class="math notranslate nohighlight">\(\partial \|u\|_1\)</span> has entries</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}g_i = \begin{cases} -1 &amp; u_i &lt; 0 \\ [-1,1] &amp; u_i = 0 \\ 1 &amp; u_i &gt; 0\end{cases}.\end{split}\]</div>
<p>For <span class="math notranslate nohighlight">\(\|u\|_2^2\)</span> we have <span class="math notranslate nohighlight">\(\partial \|u\|_2^2 = {u}\)</span>. Combining gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}g_i = \begin{cases} -\alpha + \beta u_i &amp; u_i &lt; 0 \\ [-\alpha + \beta u_i,\alpha + \beta u_i] &amp; u_i = 0 \\ \alpha + \beta u_i &amp; u_i &gt; 0\end{cases}.\end{split}\]</div>
<ul class="simple">
<li><p>We get <span class="math notranslate nohighlight">\(\partial \|Du\|_1 =  \{D^Tg\}\)</span>, with <span class="math notranslate nohighlight">\(g\)</span> given by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}g_i = \begin{cases} -1 &amp; v_i &lt; 0 \\ [-1,1] &amp; v_i = 0 \\ 1 &amp; v_i &gt; 0\end{cases},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(v = Du\)</span>.</p>
</div>
</div>
<div class="section" id="dual-problems">
<h3><span class="section-number">6.4.5. </span>Dual problems<a class="headerlink" href="#dual-problems" title="Permalink to this headline">¶</a></h3>
<p>Derive the dual problems for the following optimisation problems</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\min_u \|u - f^\delta\|_1 + \lambda \|u\|_2^2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\min_u \textstyle{\frac{1}{2}}\|u - f^\delta\|_2^2 + \lambda \|u\|_p, \quad p \in \mathbb{N}_{&gt;0}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\min_{u \in [-1,1]^n} \textstyle{\frac{1}{2}}\|u - f^\delta\|_2^2\)</span>.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>We end up with having to solve <span class="math notranslate nohighlight">\(\min_u \|u-f\|_1 + \langle \nu,u\rangle\)</span> and <span class="math notranslate nohighlight">\(\min_v \lambda \|v\|_2^2 - \langle \nu, v \rangle\)</span>. For the first, we note that the optimal value tends to <span class="math notranslate nohighlight">\(- \infty\)</span> when <span class="math notranslate nohighlight">\(\|\nu\|_{\infty} &gt; 1\)</span>. For <span class="math notranslate nohighlight">\(\|\nu\|_{\infty} \leq 1\)</span> the minimum is attained at <span class="math notranslate nohighlight">\(u = f\)</span>, giving <span class="math notranslate nohighlight">\(\langle \nu,f\rangle\)</span>.
The second one is easy, since we can compute the optimality condition easily: <span class="math notranslate nohighlight">\(2\lambda v - \nu = 0\)</span> giving <span class="math notranslate nohighlight">\(v = (2\lambda)^{-1}\nu\)</span> which leads to <span class="math notranslate nohighlight">\(-(4\lambda)^{-1}\|\nu\|_2^2\)</span>. We end up with</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\max_{\nu} \langle \nu,f \rangle - (4\lambda)^{-1}\|\nu\|_2^2 \quad \text{s.t.} \quad \|\nu\|_{\infty} \leq 1.\]</div>
<p>In terms of the optimal solution this is equivalent to</p>
<div class="math notranslate nohighlight">
\[\min_{\nu} \textstyle{\frac{1}{2}}\|\nu - \lambda f\|_2^2 \quad \text{s.t.} \quad \|\nu\|_{\infty} \leq 1.\]</div>
<ul class="simple">
<li><p>From <span class="math notranslate nohighlight">\(\min u \textstyle{\frac{1}{2}}\|u - f^\delta\|_2^2 + \langle \nu,u\rangle\)</span> we get <span class="math notranslate nohighlight">\(-\textstyle{\frac{1}{2}}\|\nu\|_2^2 + \langle \nu,f^\delta\rangle\)</span>. From <span class="math notranslate nohighlight">\(\min_{v} \lambda \|v\|_p - \langle \nu,v\rangle\)</span> we get the constraint <span class="math notranslate nohighlight">\(\|\nu\|_{q}\leq 1\)</span> with <span class="math notranslate nohighlight">\(p^{-1} + q^{-1} = 1\)</span>. This yields</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\max_{\nu} -\textstyle{\frac{1}{2}}\|\nu\|_2^2 + \langle \nu,f^\delta\rangle \quad \text{s.t.} \quad \|\nu\|_q \leq 1,\]</div>
<p>which in terms of the optimal solution is equivalent to</p>
<div class="math notranslate nohighlight">
\[\min_{\nu} \textstyle{\frac{1}{2}}\|\nu - f^\delta\|_2^2 \quad \text{s.t.} \quad \|\nu\|_q \leq 1.\]</div>
<ul class="simple">
<li><p>The saddle-point problem is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\max_{\nu} \min_{u\in\mathbb{R}^n} \textstyle{\frac{1}{2}}\|u - f^\delta\|_2^2 + \langle \nu,u \rangle + \min_v \delta_{[-1,1]^n}(v) - \langle \nu,v\rangle.\]</div>
<p>The first part is the same as the previous one. The second attains its minimum at <span class="math notranslate nohighlight">\(v_i = \text{sign}(\nu_i)\)</span> which yields <span class="math notranslate nohighlight">\(-\|\nu\|_1\)</span>. Thus the dual problem is</p>
<div class="math notranslate nohighlight">
\[\max_{\nu} -\textstyle{\frac{1}{2}}\|\nu\|_2^2 + \langle \nu,f^\delta\rangle - \|\nu\|_1,\]</div>
<p>which in terms of the optimal solution is equivalent to</p>
<div class="math notranslate nohighlight">
\[\min_{\nu} \textstyle{\frac{1}{2}}\|\nu - f^\delta\|_2^2 + \|\nu\|_1.\]</div>
</div>
</div>
<div class="section" id="tv-denoising">
<h3><span class="section-number">6.4.6. </span>TV-denoising<a class="headerlink" href="#tv-denoising" title="Permalink to this headline">¶</a></h3>
<p>In this exercise we consider a one-dimensional TV-denoising problem</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbb{R}^n} \textstyle{\frac{1}{2}} \|u - f^\delta\|_2^2 + \lambda \|Du\|_1,\]</div>
<p>with <span class="math notranslate nohighlight">\(D\)</span> a first-order finite difference discretisation of the first derivative.</p>
<ul class="simple">
<li><p>Show that the problem is equivalent (in terms of solutions) to solving</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\min_{\nu} \textstyle{\frac{1}{2}}\|D^*\nu - f^\delta\|_2^2 \quad \text{s.t.} \quad \|\nu\|_{\infty} \leq \lambda.\]</div>
<ul class="simple">
<li><p>Implement a proximal-gradient method for solving the dual problem.</p></li>
<li><p>Implement an ADMM method for solving the (primal) denoising problem.</p></li>
<li><p>Test and compare both methods on a noisy signal. Example code is given below.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>

<span class="c1"># grid \Omega = [0,1]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># parameters</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1e-1</span>

<span class="c1"># make data</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">u</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># FD differentiation matrix</span>
<span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">h</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_12_0.png" src="_images/numerical_optimisation_12_0.png" />
</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>The saddle-point problem is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\max_{\nu} \min_{u} \textstyle{\frac{1}{2}}\|u - f^\delta\|_2^2 + \langle \nu,Du\rangle + \min_v \lambda \|v\|_1 - \langle \nu,v\rangle.\]</div>
<p>See the example and previous exercise for details.</p>
<ul class="simple">
<li><p>The basic iteration for proximal-gradient is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\nu_{k+1} = \Pi_{\lambda} \left(\nu_k - \alpha D(D^*\nu_k - f^\delta)\right),\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Pi_{\lambda}(v)_i = \begin{cases} v_i &amp; |v_i| \leq \lambda \\ \lambda \text{sign}(v_i) &amp; |v_i| &gt; \lambda \end{cases}.\end{split}\]</div>
<ul class="simple">
<li><p>The basic iteration for ADMM is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[u_{k+1} = \left(I + \rho D^*\!D\right)^{-1}\left(f^\delta + D^*(\rho v_k - \nu_k)\right).\]</div>
<div class="math notranslate nohighlight">
\[v_{k+1} = \text{prox}_{(\lambda/\rho)\|\cdot\|_1}\left(Du_{k+1} + \rho^{-1}\nu_k\right).\]</div>
<div class="math notranslate nohighlight">
\[\nu_{k+1}= \nu_k + \rho \left(Du_{k+1} - v_{k+1}\right).\]</div>
<ul class="simple">
<li><p>Below we see results for both methods using <span class="math notranslate nohighlight">\(\lambda = 5\cdot 10^{-3}\)</span>, <span class="math notranslate nohighlight">\(\alpha = 1/\|D\|^2\)</span>, <span class="math notranslate nohighlight">\(\rho = 1\)</span>. Both methods give comparable results and exhibit very similar convergence behaviour.</p></li>
</ul>
<div class="figure align-default" id="tv-exercise" style="width: 500px">
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_14_0.png" src="_images/numerical_optimisation_14_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Results for TV denoising using the proximal gradient method and ADMM.</span><a class="headerlink" href="#tv-exercise" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>

<span class="k">def</span> <span class="nf">prox_grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">lmbda</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">niter</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">P</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">nu</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">nu</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">lmbda</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">nu</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">lmbda</span><span class="p">],</span> <span class="p">[</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">lmbda</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
        <span class="n">nu</span> <span class="o">=</span> <span class="n">P</span><span class="p">(</span><span class="n">nu</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">D</span><span class="o">@</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@nu</span> <span class="o">-</span> <span class="n">f</span><span class="p">))</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">f</span> <span class="o">-</span> <span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@nu</span>
        <span class="n">primal</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">lmbda</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">D</span><span class="nd">@u</span><span class="p">,</span><span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dual</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@nu</span><span class="p">)</span> <span class="o">+</span> <span class="n">f</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@nu</span><span class="p">)</span>
        <span class="n">hist</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">primal</span><span class="p">,</span> <span class="n">dual</span>

    <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">hist</span>

<span class="k">def</span> <span class="nf">admm</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">lmbda</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">rho</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">niter</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">T</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">v</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">v</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">lmbda</span><span class="o">/</span><span class="n">rho</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">lmbda</span><span class="o">/</span><span class="n">rho</span><span class="p">,</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="n">lmbda</span><span class="o">/</span><span class="n">rho</span><span class="p">],</span> <span class="p">[</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">lmbda</span><span class="o">/</span><span class="n">rho</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">lmbda</span><span class="o">/</span><span class="n">rho</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">rho</span><span class="o">*</span><span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@D</span><span class="p">,</span> <span class="n">f</span> <span class="o">+</span> <span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="o">@</span><span class="p">(</span><span class="n">rho</span><span class="o">*</span><span class="n">v</span> <span class="o">-</span> <span class="n">nu</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">T</span><span class="p">(</span><span class="n">D</span><span class="nd">@u</span> <span class="o">+</span> <span class="n">nu</span><span class="o">/</span><span class="n">rho</span><span class="p">)</span>
        <span class="n">nu</span> <span class="o">=</span> <span class="n">nu</span> <span class="o">+</span> <span class="n">rho</span><span class="o">*</span><span class="p">(</span><span class="n">D</span><span class="nd">@u</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">primal</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">lmbda</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">D</span><span class="nd">@u</span><span class="p">,</span><span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dual</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@nu</span><span class="p">)</span> <span class="o">+</span> <span class="n">f</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">T</span><span class="nd">@nu</span><span class="p">)</span>
        <span class="n">hist</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">primal</span><span class="p">,</span> <span class="n">dual</span>

    <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">hist</span>

<span class="c1"># grid \Omega = [0,1]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># parameters</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">niter</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">lmbda</span> <span class="o">=</span> <span class="mf">5e-3</span>

<span class="c1"># make data</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">u</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># FD differentiation matrix</span>
<span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="mi">0</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:])</span><span class="o">/</span><span class="n">h</span>

<span class="c1"># proximal gradient on dual problem</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">D</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">u_prox</span><span class="p">,</span> <span class="n">hist_prox</span> <span class="o">=</span> <span class="n">prox_grad</span><span class="p">(</span><span class="n">f_delta</span><span class="p">,</span><span class="n">lmbda</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="c1"># ADMM</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">u_admm</span><span class="p">,</span> <span class="n">hist_admm</span> <span class="o">=</span> <span class="n">admm</span><span class="p">(</span><span class="n">f_delta</span><span class="p">,</span><span class="n">lmbda</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">rho</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;proximal gradient&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">u_prox</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u(x)$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_prox</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;primal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_prox</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;dual&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ADMM&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">u_admm</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_admm</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;primal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_admm</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;dual&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;TV_exercise&quot;</span><span class="p">,</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_14_0.png" src="_images/numerical_optimisation_14_0.png" />
<img alt="_images/numerical_optimisation_14_1.png" src="_images/numerical_optimisation_14_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="assignments">
<h2><span class="section-number">6.5. </span>Assignments<a class="headerlink" href="#assignments" title="Permalink to this headline">¶</a></h2>
<div class="section" id="spline-regularisation">
<h3><span class="section-number">6.5.1. </span>Spline regularisation<a class="headerlink" href="#spline-regularisation" title="Permalink to this headline">¶</a></h3>
<p>The aim is to solve the following variational problem</p>
<div class="math notranslate nohighlight">
\[\min_u \frac{1}{2} \|Ku - f^{\delta}\|_2^2 + \alpha \|Lu\|_1,\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is a given forward operator (matrix) and <span class="math notranslate nohighlight">\(L\)</span> is a discretisation of the second derivative operator.</p>
<ol class="simple">
<li><p>Design and implement a method for solving this variational problem; you can be creative here – multiple answers are possible</p></li>
<li><p>Compare your method with the basic subgradient-descent method implemented below</p></li>
<li><p>(bonus) Find a suitable value for <span class="math notranslate nohighlight">\(\alpha\)</span> using the discrepancy principle</p></li>
</ol>
<p>Some code to get you started is shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>

<span class="c1"># forward operator</span>
<span class="k">def</span> <span class="nf">getK</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">h</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">h</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">xx</span> <span class="o">-</span> <span class="n">yy</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">K</span><span class="p">,</span><span class="n">x</span>

<span class="c1"># define regularization operator</span>
<span class="k">def</span> <span class="nf">getL</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">;</span>
    <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">h</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">L</span>

<span class="c1"># define grid and operators</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">K</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">getK</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">getL</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># true solution and corresponding data</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">0.5</span><span class="o">-</span><span class="n">x</span><span class="p">),</span><span class="mf">0.3</span> <span class="o">+</span> <span class="mi">0</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">K</span><span class="nd">@u</span>

<span class="c1"># noisy data</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">f</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">noise</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_16_0.png" src="_images/numerical_optimisation_16_0.png" />
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># example implementation of subgradient-descent</span>
<span class="k">def</span> <span class="nf">subgradient</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">f_delta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">niter</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="c1"># keep track of function value</span>
        <span class="n">objective</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">K</span><span class="nd">@u</span> <span class="o">-</span> <span class="n">f_delta</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="nd">@u</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># compute (sub) gradient</span>
        <span class="n">gr</span> <span class="o">=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">T</span><span class="o">@</span><span class="p">(</span><span class="n">K</span><span class="nd">@u</span> <span class="o">-</span> <span class="n">f_delta</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="nd">@np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">L</span><span class="nd">@u</span><span class="p">))</span>
        <span class="c1"># update with stepsize t</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">-</span> <span class="n">t</span><span class="o">*</span><span class="n">gr</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">objective</span>

<span class="c1"># get data</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="n">K</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">getK</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">getL</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">0.5</span><span class="o">-</span><span class="n">x</span><span class="p">),</span><span class="mf">0.3</span> <span class="o">+</span> <span class="mi">0</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">K</span><span class="nd">@u</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">noise</span>

<span class="c1"># parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">niter</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">t</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># run subgradient descent</span>
<span class="n">uhat</span><span class="p">,</span> <span class="n">objective</span> <span class="o">=</span> <span class="n">subgradient</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">f_delta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">niter</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">objective</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective value&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ground truth&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">uhat</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;reconstruction&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$u(x)$&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/numerical_optimisation_17_0.png" src="_images/numerical_optimisation_17_0.png" />
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="variational_formulations.html" title="previous page"><span class="section-number">5. </span>Variational formulations for inverse problems</a>
    <a class='right-next' id="next-link" href="image_processing.html" title="next page"><span class="section-number">1. </span>Image processing</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Tristan van Leeuwen and Christoph Brune (CC BY-NC 4.0)<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>